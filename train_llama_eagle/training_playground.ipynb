{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from safetensors import safe_open\n",
    "\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from models.llama_eagle_hf import LlamaForCausalLMEagle\n",
    "\n",
    "from datasets import Dataset, IterableDataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to download models to local dir - not sure why can't read from huggingface cached files\n",
    "\n",
    "# path = \"models/phi-3/\"\n",
    "path = \"models/llama-8b/\"\n",
    "\n",
    "with open(os.path.join(path, \"model.safetensors.index.json\"), \"r\") as f:\n",
    "  index_json = json.loads(f.read())\n",
    "  emb_path = index_json[\"weight_map\"][\"model.embed_tokens.weight\"]\n",
    "\n",
    "with safe_open(os.path.join(path, emb_path), framework=\"pt\", device=\"cpu\") as f:\n",
    "  tensor_slice = f.get_slice(\"model.embed_tokens.weight\")\n",
    "  vocab_size, hidden_dim = tensor_slice.get_shape()\n",
    "  tensor = tensor_slice[:, :hidden_dim].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_args = LlamaConfig(vocab_size=vocab_size,\n",
    "                         hidden_size=hidden_dim,\n",
    "                         intermediate_size=14336,\n",
    "                         num_hidden_layers=1,\n",
    "                         bos_token_id=128000,\n",
    "                         eos_token_id=[128001, 128008, 128009],\n",
    "                         num_key_value_heads=8,\n",
    "                         num_attention_heads=32,\n",
    "                         torch_dtype=torch.bfloat16,\n",
    "                         tie_word_embeddings=False,)\n",
    "\n",
    "draft_model = LlamaForCausalLMEagle(model_args)\n",
    "draft_model.load_embedding_weights(tensor)\n",
    "draft_model.to(\"cuda:0\")\n",
    "\n",
    "# Freeze embedding layer\n",
    "draft_model.model.embed_tokens.weight.requires_grad = False\n",
    "\n",
    "# for name, param in draft_model.named_parameters():\n",
    "#     status = \"Frozen\" if not param.requires_grad else \"Trainable\"\n",
    "#     print(f\"{name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass for draft model\n",
    "draft_model(torch.rand(1, 2, 4096, dtype=torch.bfloat16, device=\"cuda:0\"), torch.tensor([[0, 1]], device=\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5IterableDataset(IterableDataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "        self._length = None\n",
    "        self._epoch = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Each worker should get its own file handle.\n",
    "        for file_path in self.file_paths:\n",
    "            with h5py.File(file_path, \"r\") as f:\n",
    "                # Sort groups if order matters (assuming names like \"sample_0\", \"sample_1\", etc.)\n",
    "                group_keys = sorted(f.keys(), key=lambda x: int(x.split('_')[-1]))\n",
    "                for group_name in group_keys:\n",
    "                    grp = f[group_name]\n",
    "                    yield {\n",
    "                        \"input_ids\": grp[\"input_ids\"][:],       # NumPy array\n",
    "                        \"hidden_states\": grp[\"hidden_states\"][:]  # NumPy array\n",
    "                    }\n",
    "\n",
    "    def __len__(self):\n",
    "        # Compute length only once.\n",
    "        if self._length is None:\n",
    "            total = 0\n",
    "            for file_path in self.file_paths:\n",
    "                with h5py.File(file_path, \"r\") as f:\n",
    "                    total += len(f.keys())\n",
    "            self._length = total\n",
    "        return self._length\n",
    "\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self._epoch = epoch\n",
    "\n",
    "file_paths = glob.glob(\"data/train_dataset_w_hidden_states_*.h5\")\n",
    "file_paths = sorted(file_paths, key=lambda x: int(re.search(r'(\\d+)-\\d+', x).group(1)))\n",
    "\n",
    "# Instantiate your custom IterableDataset.\n",
    "eagle_dataset = HDF5IterableDataset(file_paths)\n",
    "\n",
    "# Verify length (e.g., for Trainer training steps calculation)\n",
    "print(\"Total dataset length:\", len(eagle_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_without_fast_tokenizer_warning(tokenizer, *pad_args, **pad_kwargs):\n",
    "    # Copied from huggingface padding collator\n",
    "    \"\"\"\n",
    "    Pads without triggering the warning about how using the pad function is sub-optimal when using a fast tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    # To avoid errors when using Feature extractors\n",
    "    if not hasattr(tokenizer, \"deprecation_warnings\"):\n",
    "        return tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "\n",
    "    # Save the state of the warning, then disable it\n",
    "    warning_state = tokenizer.deprecation_warnings.get(\"Asking-to-pad-a-fast-tokenizer\", False)\n",
    "    tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True\n",
    "\n",
    "    try:\n",
    "        padded = tokenizer.pad(*pad_args, **pad_kwargs)\n",
    "    finally:\n",
    "        # Restore the state of the warning.\n",
    "        tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = warning_state\n",
    "\n",
    "    return padded\n",
    "\n",
    "class EagleDataCollator:\n",
    "    \"\"\"\n",
    "    A data collator that pads variable-length sequences using the tokenizer's pad method\n",
    "    (via a helper function to avoid warnings) for text inputs and manually pads the\n",
    "    corresponding hidden states.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors=\"pt\", device=\"cuda:0\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "        self.return_tensors = return_tensors\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Expects each feature in `features` to be a dict with at least:\n",
    "           - \"input_ids\": a list of token ids,\n",
    "           - \"hidden_state\": a list or tensor of shape (seq_length, hidden_size).\n",
    "        \"\"\"\n",
    "        # Use the helper function to pad input_ids and create the attention mask.\n",
    "        # Currently right padding, might need to left pad\n",
    "\n",
    "        if type(features) is list:\n",
    "            tokenizer_features = {\"input_ids\": [el[\"input_ids\"] for el in features]}\n",
    "            hidden_states_list = [torch.tensor(el[\"hidden_states\"], dtype=torch.bfloat16) for el in features]\n",
    "        else:\n",
    "            # not sure when this is used except my own dataset[:4] batches\n",
    "            tokenizer_features = {\"input_ids\": features[\"input_ids\"]}\n",
    "            hidden_states_list = [torch.tensor(x, dtype=torch.bfloat16) for x in features[\"hidden_states\"]]\n",
    "\n",
    "        # Use the helper to pad the tokenizer features.\n",
    "        padded_batch = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            tokenizer_features,\n",
    "            padding=True,\n",
    "            max_length=None,  # or set if needed\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        hidden_states_padded = pad_sequence(hidden_states_list, batch_first=True, padding_value=0.0)\n",
    "        padded_batch[\"hidden_states\"] = hidden_states_padded\n",
    "\n",
    "        return padded_batch\n",
    "\n",
    "eagle_collator = EagleDataCollator(tokenizer, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EagleTrainer(Trainer):\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        dataloader = super().get_train_dataloader()\n",
    "        # not sure why the dataloader gets converted to MapDataloader rather than IteratorDataloader when len is available\n",
    "        dataloader.base_dataloader._dataset_kind = 1\n",
    "        return dataloader\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # TODO: add loss mask - should be computed in collator\n",
    "        # TODO: also add generation only mask, these might be the same lol\n",
    "        # TODO: add token prediction loss\n",
    "        \"\"\"\n",
    "        Uses teacher forcing with a single forward pass:\n",
    "            - For each sequence, inputs from time steps 0 to T-1 are used to predict hidden states at 1 to T.\n",
    "            - Loss is computed with an MSE loss over valid (non-padded) positions.\n",
    "        \"\"\"\n",
    "        input_ids = inputs[\"input_ids\"]           # (batch_size, padded_seq_length)\n",
    "        hidden_states = inputs[\"hidden_states\"]     # (batch_size, padded_seq_length, hidden_size)\n",
    "        attention_mask = inputs[\"attention_mask\"]   # (batch_size, padded_seq_length)\n",
    "        \n",
    "        # Teacher forcing: shift sequences.\n",
    "        input_ids_model = input_ids[:, :-1]              # (batch_size, padded_seq_length - 1)\n",
    "        hidden_states_model = hidden_states[:, :-1, :]    # (batch_size, padded_seq_length - 1, hidden_size)\n",
    "        target_hidden_states = hidden_states[:, 1:, :]     # (batch_size, padded_seq_length - 1, hidden_size)\n",
    "        # Also shift the attention mask.\n",
    "        input_attention_mask = attention_mask[:, :-1]      # For the model's input.\n",
    "        target_mask = attention_mask[:, 1:]                # For masking loss.\n",
    "        \n",
    "        # Forward pass in one go.\n",
    "        predicted_hidden_states = model(input_ids=input_ids_model,\n",
    "                                        hidden_state=hidden_states_model,\n",
    "                                        attention_mask=input_attention_mask).bfloat16()\n",
    "\n",
    "        # Compute elementwise MSE loss.\n",
    "        # loss_fct = nn.MSELoss(reduction=\"none\") # platues a little less than loss of 3\n",
    "        loss_fct = nn.SmoothL1Loss(reduction=\"none\")\n",
    "        loss_all = loss_fct(predicted_hidden_states, target_hidden_states)  # (batch, seq_len-1, hidden_size)\n",
    "        loss_all = loss_all.mean(dim=-1)  # Mean over hidden_size → (batch, seq_len-1)\n",
    "        \n",
    "        # Mask out padded positions in the target.\n",
    "        loss_all = loss_all * target_mask\n",
    "        valid_tokens = target_mask.sum()\n",
    "        loss = loss_all.sum() / valid_tokens\n",
    "        loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./hf_trainer_output_dir/\",\n",
    "    num_train_epochs=1,\n",
    "    # max_steps=750,\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    remove_unused_columns=False,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    logging_steps=10,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=500,\n",
    "    # save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = EagleTrainer(\n",
    "    model=draft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=eagle_dataset,\n",
    "    data_collator=eagle_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
